
"""
model_adapter.py
Implement send_prompt(prompt_text: str) -> str to integrate your LLM.
Swap this stub with your actual API client (OpenAI, Anthropic, local LLM, etc.).
"""
def send_prompt(prompt_text: str) -> str:
    # TODO: Replace with real model call. Keep deterministic for CI.
    return f"[MOCK OUTPUT]\\n{prompt_text[:200]}..."
